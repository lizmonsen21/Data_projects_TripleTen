{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Elizabeth!\n",
    "\n",
    "I’m happy to review your project today.\n",
    "I will mark your mistakes and give you some hints how it is possible to fix them. We are getting ready for real job, where your team leader/senior colleague will do exactly the same. Don't worry and study with pleasure! \n",
    "\n",
    "Below you will find my comments - **please do not move, modify or delete them**.\n",
    "\n",
    "You can find my comments in green, yellow or red boxes like this:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Success. Everything is done succesfully.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Remarks. Some recommendations.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Needs fixing. The block requires some corrections. Work can't be accepted with the red comments.\n",
    "</div>\n",
    "\n",
    "You can answer me by using this:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Student answer.</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Thank you so much for the feedback, I appreacaite it! I should have double checked before submitting. Thanks! \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Bank Customer Saving\n",
    "\n",
    "The purpose of this project is to develop a model based on data from Beta Bank's clients' past behavior and terminatior of contracts with the bank, to help predict if a client will terminate their contract. Identifying these customers early can help the bank try to implement measures to save their existing business as that is cheaper than attracting new ones. \n",
    "\n",
    "The data that we will be using in this analysis is as follows: \n",
    "\n",
    "Features\n",
    "- RowNumber — data string index\n",
    "- CustomerId — unique customer identifier\n",
    "- Surname — surname\n",
    "- CreditScore — credit score\n",
    "- Geography — country of residence\n",
    "- Gender — gender\n",
    "- Age — age\n",
    "- Tenure — period of maturation for a customer’s fixed deposit (years)\n",
    "- Balance — account balance\n",
    "- NumOfProducts — number of banking products used by the customer\n",
    "- HasCrCard — customer has a credit card\n",
    "- IsActiveMember — customer’s activeness\n",
    "- EstimatedSalary — estimated salary\n",
    "\n",
    "Target\n",
    "- Exited — сustomer has left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preparing the Data\n",
    "\n",
    "First we will import our data, review it and prepare it so that it is useable for our analysis and to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries that will be Needed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Download Dataset\n",
    "\n",
    "data = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "\n",
      "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
      "4881       4882    15614778  Robertson          579    France    Male   31   \n",
      "3798       3799    15621834       Game          700     Spain  Female   43   \n",
      "8302       8303    15762172       Kerr          850    France  Female   39   \n",
      "6168       6169    15644501  Enyinnaya          579    France  Female   26   \n",
      "2161       2162    15569678      Cocci          561   Germany    Male   32   \n",
      "\n",
      "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "4881     6.0       0.00              2          1               0   \n",
      "3798     0.0       0.00              2          1               0   \n",
      "8302     2.0       0.00              2          1               0   \n",
      "6168    10.0  162482.76              1          1               1   \n",
      "2161     6.0  166824.59              1          1               0   \n",
      "\n",
      "      EstimatedSalary  Exited  \n",
      "4881         26149.25       0  \n",
      "3798         59475.35       0  \n",
      "8302        179451.42       0  \n",
      "6168         18458.20       0  \n",
      "2161        139451.98       0  \n"
     ]
    }
   ],
   "source": [
    "#Briefly Review the Data\n",
    "\n",
    "data.info()\n",
    "print()\n",
    "print(data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tenure feature seemes to be the only one with missing values, but it is a relatively low percentage.  We can still review some of these values to determine if it worth dropping these missing values, filling them, or leaving them alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0     952\n",
      "2.0     950\n",
      "8.0     933\n",
      "3.0     928\n",
      "5.0     927\n",
      "7.0     925\n",
      "4.0     885\n",
      "9.0     882\n",
      "6.0     881\n",
      "10.0    446\n",
      "0.0     382\n",
      "Name: Tenure, dtype: int64\n",
      "\n",
      "909\n",
      "\n",
      "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
      "30           31    15589475    Azikiwe          591     Spain  Female   39   \n",
      "48           49    15766205        Yin          550   Germany    Male   38   \n",
      "51           52    15768193  Trevisani          585   Germany    Male   36   \n",
      "53           54    15702298   Parkhill          655   Germany    Male   41   \n",
      "60           61    15651280     Hunter          742   Germany    Male   35   \n",
      "...         ...         ...        ...          ...       ...     ...  ...   \n",
      "9944       9945    15703923    Cameron          744   Germany    Male   41   \n",
      "9956       9957    15707861      Nucci          520    France  Female   46   \n",
      "9964       9965    15642785    Douglas          479    France    Male   34   \n",
      "9985       9986    15586914     Nepean          659    France    Male   36   \n",
      "9999      10000    15628319     Walker          792    France  Female   28   \n",
      "\n",
      "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "30       NaN       0.00              3          1               0   \n",
      "48       NaN  103391.38              1          0               1   \n",
      "51       NaN  146050.97              2          0               0   \n",
      "53       NaN  125561.97              1          0               0   \n",
      "60       NaN  136857.00              1          0               0   \n",
      "...      ...        ...            ...        ...             ...   \n",
      "9944     NaN  190409.34              2          1               1   \n",
      "9956     NaN   85216.61              1          1               0   \n",
      "9964     NaN  117593.48              2          0               0   \n",
      "9985     NaN  123841.49              2          1               0   \n",
      "9999     NaN  130142.79              1          1               0   \n",
      "\n",
      "      EstimatedSalary  Exited  \n",
      "30          140469.38       1  \n",
      "48           90878.13       0  \n",
      "51           86424.57       0  \n",
      "53          164040.94       1  \n",
      "60           84509.57       0  \n",
      "...               ...     ...  \n",
      "9944        138361.48       0  \n",
      "9956        117369.52       1  \n",
      "9964        113308.29       0  \n",
      "9985         96833.00       0  \n",
      "9999         38190.78       0  \n",
      "\n",
      "[909 rows x 14 columns]\n",
      "\n",
      "0.0909\n"
     ]
    }
   ],
   "source": [
    "print(data['Tenure'].value_counts())\n",
    "print()\n",
    "print(data['Tenure'].isna().sum())\n",
    "print()\n",
    "print(data[data['Tenure'].isna()])\n",
    "print()\n",
    "print((data['Tenure'].isna().sum())/10000) #seeing what percentage of the data is NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 9% of our data is missing a value for Tenure. There doesn't seem to be a clear reason as to why these values are missing and there is still valuable data about these individuals behaviors other than how long they were with the Bank. For this reason we will fill these values with the mean or median value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    9091.000000\n",
      "mean        4.997690\n",
      "std         2.894723\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         5.000000\n",
      "75%         7.000000\n",
      "max        10.000000\n",
      "Name: Tenure, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['Tenure'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and the median are relatively close, so we will fill the missing values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "data['Tenure'].fillna(data['Tenure'].mean(), inplace=True)\n",
    "print(data['Tenure'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to prepare the data for our model. There are 3 features that are object type data: Surname, Geography and Gender. We will want to use OHE as we will be doing a linear regression model and need all values to be numerical.  To do this we will use the pd.get_dummies fxn to encode our whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith        32\n",
      "Scott        29\n",
      "Martin       29\n",
      "Walker       28\n",
      "Brown        26\n",
      "             ..\n",
      "McMasters     1\n",
      "Kincaid       1\n",
      "Vessels       1\n",
      "Gidney        1\n",
      "Huie          1\n",
      "Name: Surname, Length: 2932, dtype: int64\n",
      "France     5014\n",
      "Germany    2509\n",
      "Spain      2477\n",
      "Name: Geography, dtype: int64\n",
      "Male      5457\n",
      "Female    4543\n",
      "Name: Gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Getting an idea of the unique values in each of these features so we can get a sense of how many more features will be added\n",
    "print(data['Surname'].value_counts())\n",
    "print(data['Geography'].value_counts())\n",
    "print(data['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Do 'Surname' column is really important feature for our task? Can 'Surname' affect on our predictions? Is it worth to create 2931 columns due to this feature? Of course, no. It's not necessary to use all the features from the given dataset. You can always remove all excess features. It can not only save some computational resourses but also it can siginficantly improve model quality.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RowNumber  CustomerId  CreditScore  Age  Tenure    Balance  \\\n",
      "3382       3383    15570629          655   72     5.0  138089.97   \n",
      "669         670    15662397          640   42     5.0  176099.13   \n",
      "7218       7219    15767231          757   36     7.0  144852.06   \n",
      "334         335    15742668          626   37     6.0  108269.37   \n",
      "2746       2747    15655794          620   36     8.0       0.00   \n",
      "\n",
      "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  ...  \\\n",
      "3382              2          1               1         99920.41  ...   \n",
      "669               1          1               1          8404.73  ...   \n",
      "7218              1          0               0        130861.95  ...   \n",
      "334               1          1               0          5597.94  ...   \n",
      "2746              2          1               1        145937.99  ...   \n",
      "\n",
      "      Surname_Zotova  Surname_Zox  Surname_Zubarev  Surname_Zubareva  \\\n",
      "3382               0            0                0                 0   \n",
      "669                0            0                0                 0   \n",
      "7218               0            0                0                 0   \n",
      "334                0            0                0                 0   \n",
      "2746               0            0                0                 0   \n",
      "\n",
      "      Surname_Zuev  Surname_Zuyev  Surname_Zuyeva  Geography_Germany  \\\n",
      "3382             0              0               0                  1   \n",
      "669              0              0               0                  0   \n",
      "7218             0              0               0                  0   \n",
      "334              0              0               0                  0   \n",
      "2746             0              0               0                  0   \n",
      "\n",
      "      Geography_Spain  Gender_Male  \n",
      "3382                0            0  \n",
      "669                 0            0  \n",
      "7218                0            1  \n",
      "334                 1            0  \n",
      "2746                0            1  \n",
      "\n",
      "[5 rows x 2945 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 2945 entries, RowNumber to Gender_Male\n",
      "dtypes: float64(3), int64(8), uint8(2934)\n",
      "memory usage: 28.8 MB\n"
     ]
    }
   ],
   "source": [
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "target = data_ohe['Exited']\n",
    "features = data_ohe.drop('Exited', axis=1)\n",
    "\n",
    "print(data_ohe.sample(5))\n",
    "data_ohe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data frame has many more columns, especially as there is a very large amount of Surnames. We also dropped the original column to help mitigate the risk of us falling into the dummytrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to split our data into a training set, a validation set and a test set.  Then we will want to look at our numerical features that aren't 0,1 like our new encoded features (RowNumber, CustomerID, CreditScore, Age, Tenure, Balance, NumOfProducts, and EstimatedSalary) and standardize them so all the features are considered equally important before the algorithm execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6000 entries, 7479 to 4578\n",
      "Columns: 2945 entries, RowNumber to Gender_Male\n",
      "dtypes: float64(3), int64(8), uint8(2934)\n",
      "memory usage: 17.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 8532 to 6895\n",
      "Columns: 2945 entries, RowNumber to Gender_Male\n",
      "dtypes: float64(3), int64(8), uint8(2934)\n",
      "memory usage: 5.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 7041 to 3366\n",
      "Columns: 2945 entries, RowNumber to Gender_Male\n",
      "dtypes: float64(3), int64(8), uint8(2934)\n",
      "memory usage: 5.8 MB\n"
     ]
    }
   ],
   "source": [
    "#Creating our training, validation and test sets and identifying our numeric features\n",
    "# 1- split into training (60%) and a temp set (40%)\n",
    "data_train, data_temp = train_test_split(data_ohe, test_size=0.40, random_state=12345)\n",
    "\n",
    "# 2- split the temp set into validation (20%) and test (20%)\n",
    "data_valid, data_test = train_test_split(data_temp, test_size=0.50, random_state=12345)\n",
    "\n",
    "data_train.info()\n",
    "data_valid.info()\n",
    "data_test.info()\n",
    "\n",
    "# 3- create Training set features and target\n",
    "\n",
    "features_train= data_train.drop(['Exited'], axis = 1)\n",
    "target_train = data_train['Exited']\n",
    "\n",
    "features_valid= data_valid.drop(['Exited'], axis = 1)\n",
    "target_valid = data_valid['Exited']\n",
    "\n",
    "features_test= data_test.drop(['Exited'], axis = 1)\n",
    "target_test = data_test['Exited']\n",
    "\n",
    "numeric = ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2944)\n"
     ]
    }
   ],
   "source": [
    "#Standardized Numeric Features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Good job!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Balance of Classes\n",
    "\n",
    "Now that our data is prepared for model training we can look at the balance of our target (Exited) classes which are 0 and 1, representing if clients stayed or left. Once we determine the balance or imbalance of these classes we will train the model without taking it into account to see how things look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7963\n",
      "1    2037\n",
      "Name: Exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print value counts for our target to get an idea of the distribution of our classes\n",
    "\n",
    "print(data_ohe['Exited'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is a very significant imbalance in our classes. Based on this data it seems that significantly more clients stay than leave.  Now lets train our model without addressing this and assess it's quality based on it's F1 score and the Area Under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35024549918166936\n",
      "0.7424948130583902\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver= 'liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "f1 = f1_score(target_valid,predicted_valid)\n",
    "print(f1)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print(auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this model our F1 score is very low, meaning that our current model is fairly poor quality and won't be able to accurately predict if customers will plan on leaving soon. However, our area under the curve is fairly good as a random model would get a value of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Quality\n",
    "\n",
    "In this step we will work on fixing the class imbalance with at least 2 different approaches and run through different iterations of our model to pick the best parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will start by balancing the class weights to see if this improves the quality of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.469406392694064\n",
      "AUC-ROC: 0.736893521010894\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear', class_weight = 'balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the class weights did improve our F1 score by over 10%.  Our AUC-ROC did drop, but only slightly. This is a good sign that the overall quality of our model is improving.  Next we will try upsampling, as we are trying to find the patterns of customers who are going to leave, it will likely be benefit for our model to get more exposure to that class as it is less frequent in our current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4519774011299435\n",
      "AUC-ROC: 0.7148618731059345\n"
     ]
    }
   ],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(\n",
    "    features_train, target_train, 10\n",
    ")\n",
    "\n",
    "model = LogisticRegression (random_state = 12345, solver = 'liblinear', class_weight = 'balanced')\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that with upsampling both our F1 score and AUC-ROC go down. I even ran through multiple different iterations of the repeat number and it was highest at 1 (meaning it was not repeated. I left it at 10 for an example, but my assumption prior to running this model was incorrect and we likely will not want to incorporate upsampling into our final model. Next we will try downsampling to see if removing some of the more frequent values of clients who stayed will improve our model more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.47421093148575827\n",
      "AUC-ROC: 0.7464054343420902\n"
     ]
    }
   ],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [features_ones]\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [target_ones]\n",
    "    )\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(\n",
    "    features_train, target_train, 0.1\n",
    ")\n",
    "\n",
    "model = LogisticRegression(random_state = 12345, solver = 'liblinear', class_weight= 'balanced')\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that downsampling did improve our F1 Score and our AUC-ROC slightly, so we will keep that in our next model. Next we will try adjusting our threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.50 | Precision = 0.350, Recall = 0.737, F1 = 0.474\n",
      "Threshold = 0.52 | Precision = 0.355, Recall = 0.713, F1 = 0.474\n",
      "Threshold = 0.54 | Precision = 0.363, Recall = 0.684, F1 = 0.474\n",
      "Threshold = 0.56 | Precision = 0.373, Recall = 0.658, F1 = 0.476\n",
      "Threshold = 0.58 | Precision = 0.392, Recall = 0.648, F1 = 0.489\n",
      "Threshold = 0.60 | Precision = 0.390, Recall = 0.617, F1 = 0.478\n",
      "Threshold = 0.62 | Precision = 0.395, Recall = 0.593, F1 = 0.474\n",
      "Threshold = 0.64 | Precision = 0.404, Recall = 0.572, F1 = 0.474\n",
      "Threshold = 0.66 | Precision = 0.412, Recall = 0.545, F1 = 0.470\n",
      "Threshold = 0.68 | Precision = 0.424, Recall = 0.507, F1 = 0.462\n",
      "Threshold = 0.70 | Precision = 0.434, Recall = 0.481, F1 = 0.456\n",
      "Threshold = 0.72 | Precision = 0.444, Recall = 0.457, F1 = 0.450\n",
      "Threshold = 0.74 | Precision = 0.454, Recall = 0.435, F1 = 0.444\n",
      "Threshold = 0.76 | Precision = 0.472, Recall = 0.416, F1 = 0.442\n",
      "Threshold = 0.78 | Precision = 0.478, Recall = 0.390, F1 = 0.430\n",
      "Threshold = 0.80 | Precision = 0.484, Recall = 0.354, F1 = 0.409\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear', class_weight= 'balanced')\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "for threshold in np.arange(0.5, 0.8, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid,predicted_valid)\n",
    "    recall = recall_score(target_valid,predicted_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "\n",
    "    print(\n",
    "            'Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}, F1 = {:.3f}'.format(\n",
    "                threshold, precision, recall, f1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Good job!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our model has the best f1 score at a threshold of 0.6. But we are still not achieving our minimum F1 score of 0.59. This tells us that logistic regression is likely not the best algorithm for our model. We will switch to RandomForestClassifier and see if we can get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4844290657439447\n",
      "AUC-ROC: 0.8240999219690417\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just switching to Random Forest model improved our F1 score and our AUC-ROC. Now lets try setting our class_weight parameter to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.44563279857397503\n",
      "AUC-ROC: 0.8314659839461889\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Random Forest model this seems to lower our F1 score so we will not carry it through. Let's try to upsample with a Random Forest model. Since we created upsampled data sets we can just input those intout our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5568181818181818\n",
      "AUC-ROC: 0.8312081490935705\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did a great job increasing our F1 score to over 55%. Now we can see if changing our threshold can continue to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.30 | Precision = 0.479, Recall = 0.732, F1 = 0.579\n",
      "Threshold = 0.32 | Precision = 0.492, Recall = 0.703, F1 = 0.579\n",
      "Threshold = 0.34 | Precision = 0.512, Recall = 0.691, F1 = 0.589\n",
      "Threshold = 0.36 | Precision = 0.534, Recall = 0.658, F1 = 0.589\n",
      "Threshold = 0.38 | Precision = 0.561, Recall = 0.639, F1 = 0.597\n",
      "Threshold = 0.40 | Precision = 0.570, Recall = 0.612, F1 = 0.591\n",
      "Threshold = 0.42 | Precision = 0.580, Recall = 0.581, F1 = 0.581\n",
      "Threshold = 0.44 | Precision = 0.598, Recall = 0.545, F1 = 0.571\n",
      "Threshold = 0.46 | Precision = 0.632, Recall = 0.526, F1 = 0.574\n",
      "Threshold = 0.48 | Precision = 0.661, Recall = 0.500, F1 = 0.569\n",
      "Threshold = 0.50 | Precision = 0.685, Recall = 0.469, F1 = 0.557\n",
      "Threshold = 0.52 | Precision = 0.708, Recall = 0.440, F1 = 0.543\n",
      "Threshold = 0.54 | Precision = 0.739, Recall = 0.426, F1 = 0.540\n",
      "Threshold = 0.56 | Precision = 0.769, Recall = 0.407, F1 = 0.532\n",
      "Threshold = 0.58 | Precision = 0.773, Recall = 0.376, F1 = 0.506\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345)\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "for threshold in np.arange(0.3, 0.6, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid,predicted_valid)\n",
    "    recall = recall_score(target_valid,predicted_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "\n",
    "    print(\n",
    "            'Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}, F1 = {:.3f}'.format(\n",
    "                threshold, precision, recall, f1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Correct\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that setting our threshold to 0.38 will give us our highest F1 score of 0.597 (enough to pass). Now we can tune our hyperparameters to see if we can improve our model quality even more before our final test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best model on the validation set (n_estimators = 90): 0.5633001422475106\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(10, 100, 10): # choose hyperparameter range\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators= est) # set number of trees\n",
    "    model.fit(features_upsampled,target_upsampled) # train model on training set\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    score = f1_score(target_valid, predicted_valid) # calculate F1 score on validation set\n",
    "    if score > best_score:\n",
    "        best_score = score# save best F1 score on validation set\n",
    "        best_est = est# save number of estimators corresponding to best F1 score\n",
    "        \n",
    "print(\"F1 score of the best model on the validation set (n_estimators = {}): {}\".format(best_est, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best model on the validation set (max_depth = 90): 0.58952496954933\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_depth = 0\n",
    "for depth in range(10, 100, 10): # choose hyperparameter range\n",
    "    model = RandomForestClassifier(random_state=12345, max_depth=depth) # set number of trees\n",
    "    model.fit(features_upsampled,target_upsampled) # train model on training set\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    score = f1_score(target_valid, predicted_valid) # calculate F1 score on validation set\n",
    "    if score > best_score:\n",
    "        best_score = score# save best F1 score on validation set\n",
    "        best_depth = depth# save number of estimators corresponding to best F1 score\n",
    "        \n",
    "print(\"F1 score of the best model on the validation set (max_depth = {}): {}\".format(best_depth, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5957918050941308\n",
      "AUC-ROC: 0.8326659367646791\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators= 90, max_depth=90, class_weight = 'balanced')\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "best_threshold = 0.38\n",
    "predicted_valid = probabilities_one_valid > best_threshold\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.30 | Precision = 0.459, Recall = 0.746, F1 = 0.569\n",
      "Threshold = 0.32 | Precision = 0.490, Recall = 0.727, F1 = 0.585\n",
      "Threshold = 0.34 | Precision = 0.512, Recall = 0.701, F1 = 0.592\n",
      "Threshold = 0.36 | Precision = 0.532, Recall = 0.675, F1 = 0.595\n",
      "Threshold = 0.38 | Precision = 0.555, Recall = 0.644, F1 = 0.596\n",
      "Threshold = 0.40 | Precision = 0.583, Recall = 0.629, F1 = 0.605\n",
      "Threshold = 0.42 | Precision = 0.601, Recall = 0.598, F1 = 0.600\n",
      "Threshold = 0.44 | Precision = 0.617, Recall = 0.567, F1 = 0.591\n",
      "Threshold = 0.46 | Precision = 0.640, Recall = 0.548, F1 = 0.590\n",
      "Threshold = 0.48 | Precision = 0.660, Recall = 0.514, F1 = 0.578\n",
      "Threshold = 0.50 | Precision = 0.685, Recall = 0.478, F1 = 0.563\n",
      "Threshold = 0.52 | Precision = 0.709, Recall = 0.455, F1 = 0.554\n",
      "Threshold = 0.54 | Precision = 0.730, Recall = 0.414, F1 = 0.528\n",
      "Threshold = 0.56 | Precision = 0.784, Recall = 0.400, F1 = 0.529\n",
      "Threshold = 0.58 | Precision = 0.805, Recall = 0.385, F1 = 0.521\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators= 90, max_depth=90, class_weight = 'balanced')\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "for threshold in np.arange(0.3, 0.6, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid,predicted_valid)\n",
    "    recall = recall_score(target_valid,predicted_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "\n",
    "    print(\n",
    "            'Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}, F1 = {:.3f}'.format(\n",
    "                threshold, precision, recall, f1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6052934407364787\n",
      "AUC-ROC: 0.8326659367646791\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators= 90, max_depth=90, class_weight = 'balanced')\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "best_threshold = 0.40\n",
    "predicted_valid = probabilities_one_valid > best_threshold\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Well done!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final model gives us our best F1 score demonstrating acceptable quality with a good AUC-ROC that is well above a random model. I re-ran the threshold levels after tuning my hyperparameters, because I still wasn't getting a good enough result on the test set once we got there.  Now our model is of higher quality for our test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5905420991926182\n",
      "AUC-ROC: 0.8324166393082595\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators= 90, max_depth=90, class_weight = 'balanced')\n",
    "model.fit(features_upsampled,target_upsampled)\n",
    "final_test = model.predict(features_test)\n",
    "\n",
    "\n",
    "probabilities_test = model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "best_threshold = 0.40\n",
    "predicted_test = probabilities_one_test > best_threshold\n",
    "\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print('F1:', f1_score(target_test, predicted_test))\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n",
    "\n",
    "Great work!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we now have a model that based on our F1 score can predict if customers are going to leave Beta Bank with relatively good recall and precision.  Additionally, with an AUC-ROC of well above 0.5 we can say that this model is better than a random model and we wouldn't just be guessing that customers may leave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2862,
    "start_time": "2025-03-25T02:17:59.147Z"
   },
   {
    "duration": 26,
    "start_time": "2025-03-25T02:18:47.699Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-25T02:19:42.514Z"
   },
   {
    "duration": 14,
    "start_time": "2025-03-25T02:20:41.453Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-25T02:23:29.290Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-25T02:25:08.527Z"
   },
   {
    "duration": 2,
    "start_time": "2025-03-25T02:25:15.968Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-25T02:26:10.991Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-25T02:26:58.604Z"
   },
   {
    "duration": 11,
    "start_time": "2025-03-25T02:28:09.971Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-25T02:32:40.165Z"
   },
   {
    "duration": 186,
    "start_time": "2025-03-26T01:49:20.865Z"
   },
   {
    "duration": 2676,
    "start_time": "2025-03-26T01:49:29.294Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-26T01:49:31.972Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T01:49:31.995Z"
   },
   {
    "duration": 13,
    "start_time": "2025-03-26T01:49:32.011Z"
   },
   {
    "duration": 100,
    "start_time": "2025-03-26T02:11:46.023Z"
   },
   {
    "duration": 197,
    "start_time": "2025-03-26T02:12:40.771Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-26T02:15:54.571Z"
   },
   {
    "duration": 891,
    "start_time": "2025-03-26T02:16:01.039Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T02:16:01.932Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T02:16:01.950Z"
   },
   {
    "duration": 80,
    "start_time": "2025-03-26T02:16:01.969Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-26T02:16:02.051Z"
   },
   {
    "duration": 185,
    "start_time": "2025-03-26T02:16:02.061Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-26T02:16:59.787Z"
   },
   {
    "duration": 886,
    "start_time": "2025-03-26T02:17:06.139Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T02:17:07.027Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T02:17:07.045Z"
   },
   {
    "duration": 53,
    "start_time": "2025-03-26T02:17:07.062Z"
   },
   {
    "duration": 28,
    "start_time": "2025-03-26T02:17:07.117Z"
   },
   {
    "duration": 168,
    "start_time": "2025-03-26T02:17:07.147Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T02:31:10.594Z"
   },
   {
    "duration": 390,
    "start_time": "2025-03-26T02:31:29.140Z"
   },
   {
    "duration": 339,
    "start_time": "2025-03-26T02:34:09.743Z"
   },
   {
    "duration": 168,
    "start_time": "2025-03-26T02:34:49.096Z"
   },
   {
    "duration": 871,
    "start_time": "2025-03-26T02:34:57.627Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T02:34:58.500Z"
   },
   {
    "duration": 25,
    "start_time": "2025-03-26T02:34:58.518Z"
   },
   {
    "duration": 59,
    "start_time": "2025-03-26T02:34:58.544Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T02:34:58.605Z"
   },
   {
    "duration": 197,
    "start_time": "2025-03-26T02:34:58.614Z"
   },
   {
    "duration": 144,
    "start_time": "2025-03-26T02:34:58.813Z"
   },
   {
    "duration": 535,
    "start_time": "2025-03-26T02:36:34.683Z"
   },
   {
    "duration": 164,
    "start_time": "2025-03-26T02:37:09.455Z"
   },
   {
    "duration": 25,
    "start_time": "2025-03-26T02:37:12.543Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T02:37:59.462Z"
   },
   {
    "duration": 20,
    "start_time": "2025-03-26T02:38:33.372Z"
   },
   {
    "duration": 903,
    "start_time": "2025-03-26T02:40:10.919Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T02:40:11.824Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T02:40:11.845Z"
   },
   {
    "duration": 59,
    "start_time": "2025-03-26T02:40:11.862Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T02:40:11.942Z"
   },
   {
    "duration": 176,
    "start_time": "2025-03-26T02:40:11.951Z"
   },
   {
    "duration": 144,
    "start_time": "2025-03-26T02:40:12.129Z"
   },
   {
    "duration": 20,
    "start_time": "2025-03-26T02:40:12.275Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T03:23:55.922Z"
   },
   {
    "duration": 570,
    "start_time": "2025-03-26T03:26:50.692Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T03:28:57.331Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T03:31:09.247Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T03:31:33.870Z"
   },
   {
    "duration": 895,
    "start_time": "2025-03-26T03:31:49.424Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-26T03:31:50.322Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T03:31:50.344Z"
   },
   {
    "duration": 60,
    "start_time": "2025-03-26T03:31:50.360Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T03:31:50.423Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T03:31:50.444Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-26T03:31:50.449Z"
   },
   {
    "duration": 218,
    "start_time": "2025-03-26T03:31:50.459Z"
   },
   {
    "duration": 154,
    "start_time": "2025-03-26T03:31:50.679Z"
   },
   {
    "duration": 30,
    "start_time": "2025-03-26T03:31:50.835Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T03:31:50.866Z"
   },
   {
    "duration": 4672,
    "start_time": "2025-03-26T03:31:50.872Z"
   },
   {
    "duration": 864,
    "start_time": "2025-03-26T04:02:37.897Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T04:02:38.763Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T04:02:38.780Z"
   },
   {
    "duration": 68,
    "start_time": "2025-03-26T04:02:38.798Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T04:02:38.868Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T04:02:38.876Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T04:02:38.882Z"
   },
   {
    "duration": 192,
    "start_time": "2025-03-26T04:02:38.891Z"
   },
   {
    "duration": 195,
    "start_time": "2025-03-26T04:02:39.085Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T04:02:39.282Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T04:02:39.302Z"
   },
   {
    "duration": 3659,
    "start_time": "2025-03-26T04:02:39.308Z"
   },
   {
    "duration": 167,
    "start_time": "2025-03-26T04:03:58.301Z"
   },
   {
    "duration": 843,
    "start_time": "2025-03-26T04:04:09.596Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T04:04:10.441Z"
   },
   {
    "duration": 14,
    "start_time": "2025-03-26T04:04:10.460Z"
   },
   {
    "duration": 86,
    "start_time": "2025-03-26T04:04:10.477Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T04:04:10.565Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T04:04:10.572Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T04:04:10.578Z"
   },
   {
    "duration": 195,
    "start_time": "2025-03-26T04:04:10.588Z"
   },
   {
    "duration": 214,
    "start_time": "2025-03-26T04:04:10.785Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T04:04:11.001Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-26T04:04:11.022Z"
   },
   {
    "duration": 166,
    "start_time": "2025-03-26T04:04:11.046Z"
   },
   {
    "duration": 247,
    "start_time": "2025-03-26T04:06:48.404Z"
   },
   {
    "duration": 898,
    "start_time": "2025-03-26T04:12:01.341Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T04:12:02.241Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T04:12:02.259Z"
   },
   {
    "duration": 77,
    "start_time": "2025-03-26T04:12:02.277Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T04:12:02.356Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T04:12:02.364Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T04:12:02.371Z"
   },
   {
    "duration": 193,
    "start_time": "2025-03-26T04:12:02.379Z"
   },
   {
    "duration": 205,
    "start_time": "2025-03-26T04:12:02.573Z"
   },
   {
    "duration": 20,
    "start_time": "2025-03-26T04:12:02.782Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T04:12:02.804Z"
   },
   {
    "duration": 255,
    "start_time": "2025-03-26T04:12:02.809Z"
   },
   {
    "duration": 298,
    "start_time": "2025-03-26T04:12:03.066Z"
   },
   {
    "duration": 208,
    "start_time": "2025-03-26T04:13:57.858Z"
   },
   {
    "duration": 682,
    "start_time": "2025-03-26T04:20:53.975Z"
   },
   {
    "duration": 506,
    "start_time": "2025-03-26T04:21:14.786Z"
   },
   {
    "duration": 628,
    "start_time": "2025-03-26T04:21:20.439Z"
   },
   {
    "duration": 310,
    "start_time": "2025-03-26T04:21:25.143Z"
   },
   {
    "duration": 704,
    "start_time": "2025-03-26T04:22:10.294Z"
   },
   {
    "duration": 503,
    "start_time": "2025-03-26T04:22:21.940Z"
   },
   {
    "duration": 312,
    "start_time": "2025-03-26T04:22:28.966Z"
   },
   {
    "duration": 336,
    "start_time": "2025-03-26T04:23:34.972Z"
   },
   {
    "duration": 453,
    "start_time": "2025-03-26T04:24:18.805Z"
   },
   {
    "duration": 769,
    "start_time": "2025-03-26T04:24:27.184Z"
   },
   {
    "duration": 242,
    "start_time": "2025-03-26T04:30:10.724Z"
   },
   {
    "duration": 351,
    "start_time": "2025-03-26T04:30:34.002Z"
   },
   {
    "duration": 260,
    "start_time": "2025-03-26T04:30:44.492Z"
   },
   {
    "duration": 419,
    "start_time": "2025-03-26T04:30:51.640Z"
   },
   {
    "duration": 500,
    "start_time": "2025-03-26T04:31:04.242Z"
   },
   {
    "duration": 184,
    "start_time": "2025-03-26T04:31:11.275Z"
   },
   {
    "duration": 208,
    "start_time": "2025-03-26T04:36:51.743Z"
   },
   {
    "duration": 892,
    "start_time": "2025-03-26T04:37:36.160Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T04:37:37.055Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T04:37:37.075Z"
   },
   {
    "duration": 13,
    "start_time": "2025-03-26T04:37:37.096Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T04:37:37.112Z"
   },
   {
    "duration": 25,
    "start_time": "2025-03-26T04:37:37.121Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T04:37:37.147Z"
   },
   {
    "duration": 229,
    "start_time": "2025-03-26T04:37:37.156Z"
   },
   {
    "duration": 218,
    "start_time": "2025-03-26T04:37:37.386Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T04:37:37.606Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T04:37:37.626Z"
   },
   {
    "duration": 299,
    "start_time": "2025-03-26T04:37:37.645Z"
   },
   {
    "duration": 308,
    "start_time": "2025-03-26T04:37:37.949Z"
   },
   {
    "duration": 889,
    "start_time": "2025-03-26T04:37:38.261Z"
   },
   {
    "duration": 393,
    "start_time": "2025-03-26T04:37:39.152Z"
   },
   {
    "duration": 399,
    "start_time": "2025-03-26T04:37:39.549Z"
   },
   {
    "duration": 339,
    "start_time": "2025-03-26T04:38:59.020Z"
   },
   {
    "duration": 304,
    "start_time": "2025-03-26T04:39:57.155Z"
   },
   {
    "duration": 324,
    "start_time": "2025-03-26T04:40:19.326Z"
   },
   {
    "duration": 938,
    "start_time": "2025-03-26T04:41:50.670Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T04:41:51.610Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-26T04:41:51.626Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-26T04:41:51.648Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T04:41:51.663Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T04:41:51.670Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T04:41:51.676Z"
   },
   {
    "duration": 240,
    "start_time": "2025-03-26T04:41:51.685Z"
   },
   {
    "duration": 203,
    "start_time": "2025-03-26T04:41:51.927Z"
   },
   {
    "duration": 26,
    "start_time": "2025-03-26T04:41:52.132Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T04:41:52.160Z"
   },
   {
    "duration": 290,
    "start_time": "2025-03-26T04:41:52.167Z"
   },
   {
    "duration": 390,
    "start_time": "2025-03-26T04:41:52.460Z"
   },
   {
    "duration": 897,
    "start_time": "2025-03-26T04:41:52.853Z"
   },
   {
    "duration": 311,
    "start_time": "2025-03-26T04:41:53.752Z"
   },
   {
    "duration": 405,
    "start_time": "2025-03-26T04:41:54.067Z"
   },
   {
    "duration": 184,
    "start_time": "2025-03-26T04:46:28.893Z"
   },
   {
    "duration": 202,
    "start_time": "2025-03-26T04:46:42.956Z"
   },
   {
    "duration": 881,
    "start_time": "2025-03-26T05:03:09.044Z"
   },
   {
    "duration": 36,
    "start_time": "2025-03-26T05:03:09.927Z"
   },
   {
    "duration": 14,
    "start_time": "2025-03-26T05:03:09.965Z"
   },
   {
    "duration": 58,
    "start_time": "2025-03-26T05:03:09.981Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T05:03:10.043Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T05:03:10.051Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T05:03:10.058Z"
   },
   {
    "duration": 194,
    "start_time": "2025-03-26T05:03:10.067Z"
   },
   {
    "duration": 199,
    "start_time": "2025-03-26T05:03:10.262Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-26T05:03:10.462Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T05:03:10.487Z"
   },
   {
    "duration": 278,
    "start_time": "2025-03-26T05:03:10.492Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T05:03:10.843Z"
   },
   {
    "duration": 893,
    "start_time": "2025-03-26T05:03:11.155Z"
   },
   {
    "duration": 303,
    "start_time": "2025-03-26T05:03:12.052Z"
   },
   {
    "duration": 307,
    "start_time": "2025-03-26T05:03:12.358Z"
   },
   {
    "duration": 257,
    "start_time": "2025-03-26T05:04:52.411Z"
   },
   {
    "duration": 107,
    "start_time": "2025-03-26T05:05:07.824Z"
   },
   {
    "duration": 841,
    "start_time": "2025-03-26T05:05:20.408Z"
   },
   {
    "duration": 4734,
    "start_time": "2025-03-26T05:06:00.808Z"
   },
   {
    "duration": 4950,
    "start_time": "2025-03-26T05:06:17.295Z"
   },
   {
    "duration": 21414,
    "start_time": "2025-03-26T05:10:50.211Z"
   },
   {
    "duration": 22184,
    "start_time": "2025-03-26T05:12:11.540Z"
   },
   {
    "duration": 21288,
    "start_time": "2025-03-26T05:16:38.046Z"
   },
   {
    "duration": 905,
    "start_time": "2025-03-26T05:20:04.854Z"
   },
   {
    "duration": 17,
    "start_time": "2025-03-26T05:20:05.761Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T05:20:05.779Z"
   },
   {
    "duration": 74,
    "start_time": "2025-03-26T05:20:05.796Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T05:20:05.873Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T05:20:05.879Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T05:20:05.885Z"
   },
   {
    "duration": 195,
    "start_time": "2025-03-26T05:20:05.895Z"
   },
   {
    "duration": 199,
    "start_time": "2025-03-26T05:20:06.091Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T05:20:06.292Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T05:20:06.312Z"
   },
   {
    "duration": 246,
    "start_time": "2025-03-26T05:20:06.317Z"
   },
   {
    "duration": 388,
    "start_time": "2025-03-26T05:20:06.565Z"
   },
   {
    "duration": 885,
    "start_time": "2025-03-26T05:20:06.958Z"
   },
   {
    "duration": 309,
    "start_time": "2025-03-26T05:20:07.845Z"
   },
   {
    "duration": 307,
    "start_time": "2025-03-26T05:20:08.155Z"
   },
   {
    "duration": 4918,
    "start_time": "2025-03-26T05:20:08.464Z"
   },
   {
    "duration": 22478,
    "start_time": "2025-03-26T05:20:13.383Z"
   },
   {
    "duration": 6246,
    "start_time": "2025-03-26T05:25:21.246Z"
   },
   {
    "duration": 12770,
    "start_time": "2025-03-26T05:27:48.941Z"
   },
   {
    "duration": 12673,
    "start_time": "2025-03-26T05:31:01.628Z"
   },
   {
    "duration": 12641,
    "start_time": "2025-03-26T05:31:36.237Z"
   },
   {
    "duration": 58230,
    "start_time": "2025-03-26T05:39:59.101Z"
   },
   {
    "duration": 847,
    "start_time": "2025-03-26T05:48:55.112Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T05:48:55.961Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T05:48:55.979Z"
   },
   {
    "duration": 78,
    "start_time": "2025-03-26T05:48:55.997Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T05:48:56.077Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T05:48:56.085Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-26T05:48:56.091Z"
   },
   {
    "duration": 196,
    "start_time": "2025-03-26T05:48:56.103Z"
   },
   {
    "duration": 204,
    "start_time": "2025-03-26T05:48:56.300Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T05:48:56.506Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T05:48:56.527Z"
   },
   {
    "duration": 301,
    "start_time": "2025-03-26T05:48:56.543Z"
   },
   {
    "duration": 312,
    "start_time": "2025-03-26T05:48:56.846Z"
   },
   {
    "duration": 891,
    "start_time": "2025-03-26T05:48:57.162Z"
   },
   {
    "duration": 391,
    "start_time": "2025-03-26T05:48:58.055Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T05:48:58.448Z"
   },
   {
    "duration": 4948,
    "start_time": "2025-03-26T05:48:58.756Z"
   },
   {
    "duration": 4694,
    "start_time": "2025-03-26T05:49:03.705Z"
   },
   {
    "duration": 12741,
    "start_time": "2025-03-26T05:49:08.401Z"
   },
   {
    "duration": 13182,
    "start_time": "2025-03-26T05:49:21.145Z"
   },
   {
    "duration": 58436,
    "start_time": "2025-03-26T05:49:34.329Z"
   },
   {
    "duration": 8205,
    "start_time": "2025-03-26T05:56:03.650Z"
   },
   {
    "duration": 82868,
    "start_time": "2025-03-26T06:11:10.045Z"
   },
   {
    "duration": 72956,
    "start_time": "2025-03-26T06:13:07.515Z"
   },
   {
    "duration": 11012,
    "start_time": "2025-03-26T06:16:31.372Z"
   },
   {
    "duration": 11481,
    "start_time": "2025-03-26T06:17:21.815Z"
   },
   {
    "duration": 11273,
    "start_time": "2025-03-26T06:18:26.779Z"
   },
   {
    "duration": 98,
    "start_time": "2025-03-26T06:24:48.120Z"
   },
   {
    "duration": 885,
    "start_time": "2025-03-26T06:27:02.786Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T06:27:03.673Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T06:27:03.691Z"
   },
   {
    "duration": 69,
    "start_time": "2025-03-26T06:27:03.709Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T06:27:03.780Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T06:27:03.787Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T06:27:03.792Z"
   },
   {
    "duration": 190,
    "start_time": "2025-03-26T06:27:03.802Z"
   },
   {
    "duration": 200,
    "start_time": "2025-03-26T06:27:03.995Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T06:27:04.199Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T06:27:04.219Z"
   },
   {
    "duration": 236,
    "start_time": "2025-03-26T06:27:04.224Z"
   },
   {
    "duration": 389,
    "start_time": "2025-03-26T06:27:04.461Z"
   },
   {
    "duration": 895,
    "start_time": "2025-03-26T06:27:04.851Z"
   },
   {
    "duration": 317,
    "start_time": "2025-03-26T06:27:05.748Z"
   },
   {
    "duration": 384,
    "start_time": "2025-03-26T06:27:06.069Z"
   },
   {
    "duration": 4827,
    "start_time": "2025-03-26T06:27:06.455Z"
   },
   {
    "duration": 4699,
    "start_time": "2025-03-26T06:27:11.284Z"
   },
   {
    "duration": 12639,
    "start_time": "2025-03-26T06:27:15.985Z"
   },
   {
    "duration": 12819,
    "start_time": "2025-03-26T06:27:28.626Z"
   },
   {
    "duration": 57537,
    "start_time": "2025-03-26T06:27:41.447Z"
   },
   {
    "duration": 73497,
    "start_time": "2025-03-26T06:28:38.986Z"
   },
   {
    "duration": 10937,
    "start_time": "2025-03-26T06:29:52.484Z"
   },
   {
    "duration": 104,
    "start_time": "2025-03-26T06:30:03.422Z"
   },
   {
    "duration": 10785,
    "start_time": "2025-03-26T06:32:14.602Z"
   },
   {
    "duration": 1070,
    "start_time": "2025-03-26T06:34:49.742Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T06:34:50.815Z"
   },
   {
    "duration": 24,
    "start_time": "2025-03-26T06:34:50.832Z"
   },
   {
    "duration": 89,
    "start_time": "2025-03-26T06:34:50.858Z"
   },
   {
    "duration": 11,
    "start_time": "2025-03-26T06:34:50.951Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T06:34:50.964Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-26T06:34:50.970Z"
   },
   {
    "duration": 175,
    "start_time": "2025-03-26T06:34:50.980Z"
   },
   {
    "duration": 202,
    "start_time": "2025-03-26T06:34:51.157Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T06:34:51.360Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T06:34:51.381Z"
   },
   {
    "duration": 271,
    "start_time": "2025-03-26T06:34:51.386Z"
   },
   {
    "duration": 390,
    "start_time": "2025-03-26T06:34:51.659Z"
   },
   {
    "duration": 808,
    "start_time": "2025-03-26T06:34:52.056Z"
   },
   {
    "duration": 384,
    "start_time": "2025-03-26T06:34:52.866Z"
   },
   {
    "duration": 304,
    "start_time": "2025-03-26T06:34:53.253Z"
   },
   {
    "duration": 4936,
    "start_time": "2025-03-26T06:34:53.560Z"
   },
   {
    "duration": 4712,
    "start_time": "2025-03-26T06:34:58.498Z"
   },
   {
    "duration": 12624,
    "start_time": "2025-03-26T06:35:03.212Z"
   },
   {
    "duration": 13133,
    "start_time": "2025-03-26T06:35:15.842Z"
   },
   {
    "duration": 12756,
    "start_time": "2025-03-26T06:35:28.977Z"
   },
   {
    "duration": 57333,
    "start_time": "2025-03-26T06:35:41.735Z"
   },
   {
    "duration": 72936,
    "start_time": "2025-03-26T06:36:39.070Z"
   },
   {
    "duration": 10967,
    "start_time": "2025-03-26T06:37:52.008Z"
   },
   {
    "duration": 10844,
    "start_time": "2025-03-26T06:38:02.977Z"
   },
   {
    "duration": 860,
    "start_time": "2025-03-26T06:41:47.392Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T06:41:48.255Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T06:41:48.275Z"
   },
   {
    "duration": 82,
    "start_time": "2025-03-26T06:41:48.292Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T06:41:48.377Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T06:41:48.384Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T06:41:48.390Z"
   },
   {
    "duration": 190,
    "start_time": "2025-03-26T06:41:48.400Z"
   },
   {
    "duration": 458,
    "start_time": "2025-03-26T06:41:48.592Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-26T06:41:49.051Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T06:41:49.071Z"
   },
   {
    "duration": 271,
    "start_time": "2025-03-26T06:41:49.077Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T06:41:49.350Z"
   },
   {
    "duration": 884,
    "start_time": "2025-03-26T06:41:49.660Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T06:41:50.546Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T06:41:50.855Z"
   },
   {
    "duration": 4829,
    "start_time": "2025-03-26T06:41:51.163Z"
   },
   {
    "duration": 4715,
    "start_time": "2025-03-26T06:41:55.994Z"
   },
   {
    "duration": 12771,
    "start_time": "2025-03-26T06:42:00.711Z"
   },
   {
    "duration": 12729,
    "start_time": "2025-03-26T06:42:13.484Z"
   },
   {
    "duration": 57171,
    "start_time": "2025-03-26T06:42:26.215Z"
   },
   {
    "duration": 896,
    "start_time": "2025-03-26T06:43:58.909Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T06:43:59.808Z"
   },
   {
    "duration": 18,
    "start_time": "2025-03-26T06:43:59.825Z"
   },
   {
    "duration": 53,
    "start_time": "2025-03-26T06:43:59.845Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-26T06:43:59.901Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-26T06:43:59.908Z"
   },
   {
    "duration": 33,
    "start_time": "2025-03-26T06:43:59.914Z"
   },
   {
    "duration": 174,
    "start_time": "2025-03-26T06:43:59.948Z"
   },
   {
    "duration": 483,
    "start_time": "2025-03-26T06:44:00.124Z"
   },
   {
    "duration": 33,
    "start_time": "2025-03-26T06:44:00.610Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T06:44:00.646Z"
   },
   {
    "duration": 216,
    "start_time": "2025-03-26T06:44:00.651Z"
   },
   {
    "duration": 304,
    "start_time": "2025-03-26T06:44:00.943Z"
   },
   {
    "duration": 896,
    "start_time": "2025-03-26T06:44:01.251Z"
   },
   {
    "duration": 310,
    "start_time": "2025-03-26T06:44:02.149Z"
   },
   {
    "duration": 311,
    "start_time": "2025-03-26T06:44:02.461Z"
   },
   {
    "duration": 4856,
    "start_time": "2025-03-26T06:44:02.775Z"
   },
   {
    "duration": 4667,
    "start_time": "2025-03-26T06:44:07.633Z"
   },
   {
    "duration": 12672,
    "start_time": "2025-03-26T06:44:12.302Z"
   },
   {
    "duration": 12603,
    "start_time": "2025-03-26T06:44:24.976Z"
   },
   {
    "duration": 57250,
    "start_time": "2025-03-26T06:44:37.581Z"
   },
   {
    "duration": 73104,
    "start_time": "2025-03-26T06:45:34.833Z"
   },
   {
    "duration": 10907,
    "start_time": "2025-03-26T06:46:47.938Z"
   },
   {
    "duration": 10943,
    "start_time": "2025-03-26T06:46:58.847Z"
   },
   {
    "duration": 13570,
    "start_time": "2025-03-26T06:50:52.457Z"
   },
   {
    "duration": 13429,
    "start_time": "2025-03-26T06:51:27.580Z"
   },
   {
    "duration": 13368,
    "start_time": "2025-03-26T06:53:13.741Z"
   },
   {
    "duration": 890,
    "start_time": "2025-03-26T06:53:52.128Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-26T06:53:53.020Z"
   },
   {
    "duration": 15,
    "start_time": "2025-03-26T06:53:53.043Z"
   },
   {
    "duration": 54,
    "start_time": "2025-03-26T06:53:53.060Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-26T06:53:53.118Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-26T06:53:53.145Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-26T06:53:53.150Z"
   },
   {
    "duration": 180,
    "start_time": "2025-03-26T06:53:53.160Z"
   },
   {
    "duration": 465,
    "start_time": "2025-03-26T06:53:53.342Z"
   },
   {
    "duration": 23,
    "start_time": "2025-03-26T06:53:53.809Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-26T06:53:53.834Z"
   },
   {
    "duration": 297,
    "start_time": "2025-03-26T06:53:53.846Z"
   },
   {
    "duration": 306,
    "start_time": "2025-03-26T06:53:54.146Z"
   },
   {
    "duration": 892,
    "start_time": "2025-03-26T06:53:54.455Z"
   },
   {
    "duration": 304,
    "start_time": "2025-03-26T06:53:55.350Z"
   },
   {
    "duration": 309,
    "start_time": "2025-03-26T06:53:55.656Z"
   },
   {
    "duration": 4798,
    "start_time": "2025-03-26T06:53:55.967Z"
   },
   {
    "duration": 4721,
    "start_time": "2025-03-26T06:54:00.767Z"
   },
   {
    "duration": 12603,
    "start_time": "2025-03-26T06:54:05.490Z"
   },
   {
    "duration": 12662,
    "start_time": "2025-03-26T06:54:18.095Z"
   },
   {
    "duration": 57096,
    "start_time": "2025-03-26T06:54:30.759Z"
   },
   {
    "duration": 72882,
    "start_time": "2025-03-26T06:55:27.856Z"
   },
   {
    "duration": 13451,
    "start_time": "2025-03-26T06:56:40.743Z"
   },
   {
    "duration": 13479,
    "start_time": "2025-03-26T06:56:54.195Z"
   },
   {
    "duration": 13619,
    "start_time": "2025-03-26T06:57:07.676Z"
   },
   {
    "duration": 13902,
    "start_time": "2025-03-26T06:58:28.057Z"
   },
   {
    "duration": 13563,
    "start_time": "2025-03-26T06:58:53.672Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-26T09:13:13.548Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
